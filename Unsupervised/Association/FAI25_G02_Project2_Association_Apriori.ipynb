{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fundamentals of Artificial Intelligence**\n",
        "\n",
        "## MSc in Applied Artificial Intelligence 2025/2026 <br>\n",
        "## Group 02 - Project 2 - Apriori\n",
        "| Nome                              | Número de Aluno |\n",
        "|-----------------------------------|------------:|\n",
        "| Adelino Daniel da Rocha Vilaça    | a16939          |\n",
        "| António Jorge Magalhães da Rocha  | a26052          |"
      ],
      "metadata": {
        "id": "W6XMPuw6RejA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 0. - **INTRODUCTION**"
      ],
      "metadata": {
        "id": "RKf6vA2eRniW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5069c23"
      },
      "source": [
        "## 0.1 - Goal\n",
        "> The primary goal of this project is to apply the Apriori algorithm to a cybersecurity intrusion detection dataset to discover meaningful association rules. These rules aim to identify common patterns, relationships, and sequences of events that lead to or are characteristic of cyber-attacks. By uncovering these associations, we seek to provide actionable insights that can enhance intrusion detection systems, improve threat intelligence, and inform security incident response strategies.\n",
        "\n",
        "## 0.2 - Environment\n",
        "> This project is developed within a Google Colaboratory environment, leveraging its cloud-based Jupyter Notebook infrastructure. Key Python libraries utilized include `pandas` for data manipulation, and `mlxtend` for the implementation of the Apriori algorithm and association rule generation. The environment provides access to necessary computational resources and facilitates collaborative development.\n",
        "\n",
        "## 0.3 - Definitions\n",
        "> - **Association Rule Mining**: A data mining technique used to find strong relationships between items in large datasets. It identifies frequently occurring itemsets and derives implication rules.\n",
        "> - **Apriori Algorithm**: An influential algorithm for mining frequent itemsets and relevant association rules. It operates on the principle that if an itemset is frequent, then all of its subsets must also be frequent.\n",
        "> - **Support**: A measure of how frequently an itemset appears in the dataset, calculated as the proportion of transactions containing the itemset.\n",
        "> - **Confidence**: A measure of the reliability of an association rule, indicating how often items in the consequent appear in transactions that already contain the antecedent.\n",
        "> - **Lift**: A metric that compares the frequency of occurrence of the antecedent and consequent together with the frequency with which they would occur if they were independent. A lift greater than 1 indicates a positive correlation between the items."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1. - **AGENT DESIGN**"
      ],
      "metadata": {
        "id": "pZ_rDELWRrd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 - Platforms"
      ],
      "metadata": {
        "id": "IvgRpOx9RtBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 - Jupyter Notebook <br>\n",
        " > A Jupyter Notebook is an open-source web application that allows creating and sharing documents containing live code, equations, visualizations, and narrative text. It's widely used in data science, machine learning, and scientific computing for interactive development, exploration, and documentation.\n",
        "\n",
        "### 1.1.2 - Google Colaboratory <br>\n",
        "  >Free cloud-based service that provides a hosted Jupyter Notebook environment. It allows writing and executing code in a browser for free and without any setup."
      ],
      "metadata": {
        "id": "3-Ea5dpGRuf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Packages and Libraries\n"
      ],
      "metadata": {
        "id": "7raLfDBgRwBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apriori python libraries\n",
        "\n",
        "**mlxtend**: Implements manny machine learning algorithms and tools, including association rule mining.\n",
        "\n",
        "**apyori**: Provides functions for manipulating transactional data and for generating association rules and evaluating their quality.\n",
        "\n",
        "**PyCaret**: Low-code ML library for automating machine learning workflows. It provides a wrapper on top of mlxtend for easy implementation of the Apriori algorithm. Current version (3.2.0) does not support association rules. Find more in https://pycaret.org/.\n"
      ],
      "metadata": {
        "id": "YgPXyrzs9o1E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning - APRIORI\n",
        "\n",
        "This notebook presents examples of the use of the well-known Apriori learning algorithms.\n"
      ],
      "metadata": {
        "id": "R-9_2Haa3Nln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the data"
      ],
      "metadata": {
        "id": "2eLC8tFTaDI5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627fb0c4"
      },
      "source": [
        "### Usage of `kaggle.json`\n",
        "\n",
        "The `kaggle.json` file serves as an essential **authentication token** for interacting with the Kaggle API (Kaggle Application Programming Interface). It securely stores the user's Kaggle credentials, including their username and key."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # kaggle.json"
      ],
      "metadata": {
        "id": "P7kytrCkpTCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Permissions"
      ],
      "metadata": {
        "id": "oO7LpjTxeorE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "dE4VHDzud0pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets list | head"
      ],
      "metadata": {
        "id": "gVqBil5wd6wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d dnkumars/cybersecurity-intrusion-detection-dataset -p /content/ --unzip"
      ],
      "metadata": {
        "id": "65QA6Q7LeJZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "from mlxtend.frequent_patterns import apriori, association_rules"
      ],
      "metadata": {
        "id": "qWKRkYe9apRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f03be295"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 DATASET\n",
        "\n",
        "\n",
        "### 1.3.1 Used Dataset\n",
        "\n",
        "> This dataset, named `cybersecurity_intrusion_data.csv`, focuses on **cybersecurity intrusion detection**.\n",
        "\n",
        "* It contains information about network sessions, such as packet size, protocol type, session duration, and login attempts.\n",
        "* It includes details about user behavior, such as browser type and failed login attempts.\n",
        "* The main objective is to classify whether a given session indicates an `attack_detected` (attack detected) or not, with this being the target variable."
      ],
      "metadata": {
        "id": "FkMZuZYt7ZgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. **AGENT RUNNING**"
      ],
      "metadata": {
        "id": "fzu280byR1dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the file\n",
        "dataset = pd.read_csv('cybersecurity_intrusion_data.csv')\n",
        "df = dataset\n",
        "\n",
        "# We will use 'df' as the base for our Apriori analysis.\n",
        "apriori_df = df.copy()\n",
        "\n",
        "# No retail-specific data preparation needed here\n",
        "\n",
        "# Print head\n",
        "apriori_df.tail()"
      ],
      "metadata": {
        "id": "WrH2ECizBoYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of nulls in each column\n",
        "print(apriori_df.isnull().sum())"
      ],
      "metadata": {
        "id": "3RSytZ6OGrP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the rows with null values\n",
        "apriori_df = apriori_df.dropna()\n",
        "\n",
        "# No Date filtering is needed for this dataset"
      ],
      "metadata": {
        "id": "QsHYY2hVHXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of rows\n",
        "print(\"Number of rows:\", len(apriori_df))\n",
        "\n",
        "# Number of distinct values\n",
        "print(\"\\nNumber of distinct values:\")\n",
        "print(\"session_id:         \", apriori_df['session_id'].nunique())\n",
        "print(\"protocol_type:      \", apriori_df['protocol_type'].nunique())\n",
        "print(\"encryption_used:    \", apriori_df['encryption_used'].nunique())\n",
        "print(\"browser_type:       \", apriori_df['browser_type'].nunique())\n",
        "print(\"unusual_time_access:\", apriori_df['unusual_time_access'].nunique())\n",
        "print(\"attack_detected:    \", apriori_df['attack_detected'].nunique())"
      ],
      "metadata": {
        "id": "zHhWBtCCFyZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pivot table\n",
        "For using the apriori algorithm we need to pivot the table.\n",
        "\n",
        "If the product is in the invoice, the intersection cell will be “True”. If is not, it will be “False”."
      ],
      "metadata": {
        "id": "dBktoDroIhB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The 'ds_grouped' DataFrame is now prepared in the cell above (H2Kv1w2zIf1k)\n",
        "# with the transactional data from our cybersecurity dataset.\n",
        "# We will use this 'ds_grouped' for the pivot table in the next step."
      ],
      "metadata": {
        "id": "DNFopNkClD9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5f8b6ee"
      },
      "source": [
        "# Prepare data for Apriori: Discretization and creating transaction lists\n",
        "apriori_df_processed = apriori_df.copy()\n",
        "\n",
        "# Discretize 'failed_logins' based on the suggestion: \"0\", \"1\", \"2\", \"3+\"\n",
        "bins_failed_logins = [-0.1, 0, 1, 2, apriori_df_processed['failed_logins'].max() + 1]\n",
        "labels_failed_logins = ['failed_logins_0', 'failed_logins_1', 'failed_logins_2', 'failed_logins_3+']\n",
        "apriori_df_processed['failed_logins_bin'] = pd.cut(\n",
        "    apriori_df_processed['failed_logins'],\n",
        "    bins=bins_failed_logins,\n",
        "    labels=labels_failed_logins,\n",
        "    right=False,\n",
        "    include_lowest=True\n",
        ")\n",
        "\n",
        "# Discretize other numerical attributes using quantiles (e.g., 4 bins for now)\n",
        "numerical_cols_to_bin = [\n",
        "    'network_packet_size',\n",
        "    'login_attempts',\n",
        "    'session_duration',\n",
        "    'ip_reputation_score'\n",
        "]\n",
        "\n",
        "for col in numerical_cols_to_bin:\n",
        "    try:\n",
        "        # Create 4 quantile-based bins\n",
        "        apriori_df_processed[f'{col}_bin'] = pd.qcut(\n",
        "            apriori_df_processed[col],\n",
        "            q=4,\n",
        "            duplicates='drop', # Handle cases with identical values across quantiles\n",
        "            labels=[f'{col}_Q1', f'{col}_Q2', f'{col}_Q3', f'{col}_Q4']\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(f\"Warning: Could not qcut column '{col}' due to: {e}. Falling back to equal-width binning.\")\n",
        "        apriori_df_processed[f'{col}_bin'] = pd.cut(\n",
        "            apriori_df_processed[col],\n",
        "            bins=4,\n",
        "            labels=[f'{col}_B1', f'{col}_B2', f'{col}_B3', f'{col}_B4'],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "# Identify all columns that will serve as 'items' for association rule mining\n",
        "# Exclude 'session_id' and original numerical columns\n",
        "item_columns = [\n",
        "    'protocol_type',\n",
        "    'encryption_used',\n",
        "    'browser_type',\n",
        "    'unusual_time_access',\n",
        "    'attack_detected',\n",
        "    'failed_logins_bin',\n",
        "    'network_packet_size_bin',\n",
        "    'login_attempts_bin',\n",
        "    'session_duration_bin',\n",
        "    'ip_reputation_score_bin'\n",
        "]\n",
        "\n",
        "# Ensure all identified item_columns exist in the DataFrame\n",
        "item_columns = [col for col in item_columns if col in apriori_df_processed.columns]\n",
        "\n",
        "# Convert all item columns to string to prepare for item list creation\n",
        "for col in item_columns:\n",
        "    apriori_df_processed[col] = apriori_df_processed[col].astype(str)\n",
        "\n",
        "\n",
        "# Structure grouped data\n",
        "transaction_items = []\n",
        "for index, row in apriori_df_processed.iterrows():\n",
        "    session_id = row['session_id']\n",
        "    for col in item_columns:\n",
        "        item_value = row[col]\n",
        "        # Create a unique item string, e.g., 'protocol_type_TCP', 'browser_type_Chrome'\n",
        "        transaction_items.append({'session_id': session_id, 'item': f'{col}_{item_value}'})\n",
        "\n",
        "ds_grouped = pd.DataFrame(transaction_items)\n",
        "\n",
        "# Display the head of this 'grouped' data\n",
        "ds_grouped.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create apriori data structure\n",
        "ds_pivot = ds_grouped.pivot_table(index='session_id', columns='item', aggfunc=lambda x: True, fill_value=False)\n",
        "ds_pivot.tail(5)"
      ],
      "metadata": {
        "id": "mGYuUkuAI20v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Learning Rules (Association Rule Learning)"
      ],
      "metadata": {
        "id": "XKH8kgHHKst4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the rules\n",
        "min_support=0.01\n",
        "freq_itemsets = apriori(ds_pivot, min_support=min_support, use_colnames=True)\n",
        "\n",
        "# Get the number of itemsets in freq_itemsets\n",
        "num_itemsets = len(freq_itemsets)\n",
        "print(f'Number of itemsets: {num_itemsets}')\n",
        "display(freq_itemsets.head())\n",
        "\n",
        "# Get the rules\n",
        "rules = association_rules(freq_itemsets, metric=\"support\", min_threshold=min_support)\n",
        "display(rules.sort_values('support', ascending=False).head(10))"
      ],
      "metadata": {
        "id": "CiNOPfFrKtLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List the 10 rules with higher confidence\n",
        "rules.sort_values('confidence', ascending=False).head(10)"
      ],
      "metadata": {
        "id": "gSSOhLblJb4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rules_for_item(item_name):\n",
        "\n",
        "    # Filter rules where the specified item is in the antecedents\n",
        "    # Check if the item_name (string) is contained within any of the item strings in the frozenset\n",
        "    filtered_rules = rules[rules['antecedents'].apply(lambda x: any(item_name == str(i) for i in x))]\n",
        "\n",
        "    # Prepare the output\n",
        "    results = []\n",
        "    for index, row in filtered_rules.iterrows():\n",
        "        rule_info = {}\n",
        "        # Items are already descriptive, so we convert frozenset to a comma separated string\n",
        "        rule_info['antecedents'] = \", \".join(map(str, row['antecedents']))\n",
        "        rule_info['consequents'] = \", \".join(map(str, row['consequents']))\n",
        "        rule_info['support'] = row['support']\n",
        "        rule_info['confidence'] = row['confidence']\n",
        "        results.append(rule_info)\n",
        "\n",
        "    # Convert the results to a DataFrame and sort by confidence\n",
        "    result_df = pd.DataFrame(results)\n",
        "    if not result_df.empty:\n",
        "        result_df = result_df.sort_values('confidence', ascending=False).head(10)\n",
        "\n",
        "    return result_df\n",
        "\n",
        "# Example usage with a relevant item from our dataset (e.g., 'protocol_type_TCP')\n",
        "example_item = 'protocol_type_TCP'\n",
        "product_rules = get_rules_for_item(example_item)\n",
        "product_rules"
      ],
      "metadata": {
        "id": "TbadJgBmPzTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4d91b13"
      },
      "source": [
        "### Inspecting Rules Related to Attack Detection\n",
        "\n",
        "Since our project focuses on cybersecurity intrusion detection, let's specifically look for association rules where an `attack_detected_1` (meaning an attack was detected) is the consequent. These rules can be highly valuable for identifying patterns that precede or are strongly associated with security incidents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14b08df7"
      },
      "source": [
        "# Filter rules where 'attack_detected_1' is in the consequents\n",
        "attack_detection_rules = rules[rules['consequents'].apply(lambda x: 'attack_detected_1' in x)]\n",
        "\n",
        "# Sort these rules by lift and then by confidence to find the most interesting ones\n",
        "attack_detection_rules_sorted = attack_detection_rules.sort_values(\n",
        "    by=['lift', 'confidence'], ascending=False\n",
        ")\n",
        "\n",
        "print(f\"Number of rules where 'attack_detected_1' is a consequent: {len(attack_detection_rules)}\")\n",
        "\n",
        "# Display the top 10 rules related to attack detection\n",
        "display(attack_detection_rules_sorted.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a4f6520"
      },
      "source": [
        "#### Interpretation of `attack_detection_rules_sorted`:\n",
        "\n",
        "*   **High Lift**: Close attention to rules with a `lift` significantly greater than 1. These are the patterns where the `antecedents` are much more likely to occur *together with* `attack_detected_1` than would be expected by chance alone. This indicates a strong, potentially interesting relationship.\n",
        "*   **High Confidence**: Alongside `lift`, `confidence` tells you how reliable the prediction is. A high confidence means that when the `antecedents` are present, `attack_detected_1` is very likely to also be present.\n",
        "*   **Support**: While less critical for filtering than lift and confidence in this context, support still indicates how frequently this specific pattern occurs in the dataset. You might find some highly predictive rules (`high lift`, `high confidence`) that have lower support, meaning they are rarer but still important indicators when they do occur.\n",
        "\n",
        "These rules can help us understand which combinations of factors (e.g., protocol type, browser, login attempts, IP reputation) are most strongly associated with attack detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3de62fb"
      },
      "source": [
        "### Visualization of Association Rules\n",
        "\n",
        "A scatter plot is a powerful way to visualize association rules, allowing us to simultaneously observe the relationships between `support`, `confidence`, and `lift`. This helps in identifying rules that are frequent, reliable, and have a strong, non-random association."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b63afdd2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filter rules to make the plot more readable (e.g., lift > 1 for positive association)\n",
        "# filtered_rules = rules[rules['lift'] > 1]\n",
        "filtered_rules = rules[(rules['lift'] > 1.2) & (rules['confidence'] > 0.7)]\n",
        "\n",
        "plt.figure(figsize=(14, 9))\n",
        "scatter = sns.scatterplot(\n",
        "    x='support',\n",
        "    y='confidence',\n",
        "    size='lift',\n",
        "    hue='lift',\n",
        "    data=filtered_rules,\n",
        "    palette='viridis',\n",
        "    sizes=(50, 600), # Adjust size range as needed\n",
        "    legend='brief'\n",
        ")\n",
        "\n",
        "plt.title('Association Rules: Support vs. Confidence (Colored by Lift)')\n",
        "plt.xlabel('Support')\n",
        "plt.ylabel('Confidence')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "\n",
        "# Rule with the highest lift:\n",
        "# max_lift_rule = filtered_rules.loc[filtered_rules['lift'].idxmax()]\n",
        "# plt.annotate(f'Max Lift: {max_lift_rule.name}',\n",
        "#              (max_lift_rule['support'], max_lift_rule['confidence']),\n",
        "#              textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44235a89"
      },
      "source": [
        "### Example Aproach\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "top_rules_scatterplot"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure filtered_rules is defined (from previous cell, lift > 1)\n",
        "if 'filtered_rules' not in locals():\n",
        "    print(\"Warning: 'filtered_rules' DataFrame not found. Please ensure previous cells are run.\")\n",
        "    # Fallback if filtered_rules is not available (though it should be if prior cells run)\n",
        "    filtered_rules = rules[rules['lift'] > 1]\n",
        "\n",
        "# Sort by lift and then confidence to get the most interesting rules\n",
        "top_rules_for_plot = filtered_rules.sort_values(by=['lift', 'confidence'], ascending=False).head(1000)\n",
        "\n",
        "plt.figure(figsize=(14, 9))\n",
        "scatter = sns.scatterplot(\n",
        "    x='support',\n",
        "    y='confidence',\n",
        "    size='lift',\n",
        "    hue='lift',\n",
        "    data=top_rules_for_plot,\n",
        "    palette='viridis',\n",
        "    sizes=(50, 600),\n",
        "    legend='brief'\n",
        ")\n",
        "\n",
        "plt.title('Top 1000 Association Rules: Support vs. Confidence (Colored by Lift)')\n",
        "plt.xlabel('Support')\n",
        "plt.ylabel('Confidence')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# Move the legend outside the plot area to the right\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "\n",
        "# Optionally, add annotations for the very top rules if desired\n",
        "# for i, row in top_rules_for_plot.head(5).iterrows():\n",
        "#     plt.annotate(f'Lift: {row['lift']:.2f}', (row['support'], row['confidence']),\n",
        "#                  textcoords=\"offset points\", xytext=(5,5), ha='left', fontsize=8)\n",
        "\n",
        "plt.show()\n",
        "print(f\"Plotted {len(top_rules_for_plot)} rules.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4b3c0a5"
      },
      "source": [
        "## 3.1 Overall\n",
        "> This project successfully applied the Apriori algorithm to identify significant association rules within a cybersecurity intrusion detection dataset. We processed raw data, handled missing values, and discretized numerical features to transform the dataset into a transactional format suitable for Apriori. The analysis revealed various patterns, particularly highlighting conditions strongly associated with attack detection, providing valuable insights for security monitoring and incident response.\n",
        "\n",
        "## 3.2 Challenges and solutions\n",
        "> One key challenge was the transformation of numerical and categorical data into a format compatible with association rule mining. This was addressed by discretizing numerical columns into meaningful bins (e.g., quantiles for 'network_packet_size' and specific ranges for 'failed_logins') and encoding categorical variables. Another challenge was the interpretation of a large number of generated rules; this was mitigated by filtering and sorting rules based on metrics like lift and confidence, especially focusing on rules where 'attack_detected_1' was the consequent.\n",
        "\n",
        "## 3.3 Looking forward\n",
        "> Future work could involve exploring different binning strategies for numerical data to see how it impacts rule generation. Additionally, integrating other advanced association rule mining algorithms (e.g., FP-Growth for potentially larger datasets) could be beneficial. Further investigation into the specific antecedents leading to attack detection, perhaps by domain experts, could lead to more actionable security intelligence.\n",
        "\n",
        "## 3.4 In hindsight\n",
        "> Reflecting on the project, the initial data preparation phase, particularly the discretization of continuous variables, was more critical than anticipated for generating coherent and interpretable rules. Understanding the domain context thoroughly before defining bins for numerical data would have streamlined the process. The visualization of association rules proved invaluable for quickly grasping the most significant patterns rather than sifting through a large tabular output.\n",
        "\n",
        "<br>\n",
        "\n",
        "Thank you, Professor Joaquim :)"
      ]
    }
  ]
}